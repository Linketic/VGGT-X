{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import io\n",
    "import evo\n",
    "import evo.main_ape as main_ape\n",
    "import evo.main_rpe as main_rpe\n",
    "\n",
    "from tqdm import tqdm\n",
    "from evo.core.metrics import PoseRelation, Unit\n",
    "from evo.core.trajectory import PoseTrajectory3D\n",
    "from evo.core import lie_algebra\n",
    "from evo.tools.plot import PlotMode\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.transform import Rotation\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images_ratio\n",
    "from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "from vggt.utils.helper import create_pixel_coordinate_grid, randomly_limit_trues\n",
    "from vggt.dependency.track_predict import predict_tracks\n",
    "from vggt.dependency.np_to_pycolmap import batch_np_matrix_to_pycolmap, batch_np_matrix_to_pycolmap_wo_track\n",
    "\n",
    "from utils.umeyama import umeyama\n",
    "from utils.cam_viz import create_interactive_camera_animation\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    --  Umeyama Scale:  0.49223298431022267\n",
      "    --  Umeyama Rotation: \n",
      " [[ 0.64462238 -0.27088347  0.71490148]\n",
      " [-0.00804973  0.93266472  0.36065458]\n",
      " [-0.76445875 -0.23824078  0.59903601]]\n",
      "    --  Umeyama Translation: \n",
      " [[-0.38132978]\n",
      " [-1.35942802]\n",
      " [ 0.30531174]]\n",
      "Skipping flowers as required files are missing.\n",
      "    --  Umeyama Scale:  0.4198601359739963\n",
      "    --  Umeyama Rotation: \n",
      " [[ 0.32209217 -0.05297554 -0.94522496]\n",
      " [-0.19672005  0.97289484 -0.12156008]\n",
      " [ 0.9260442   0.22509825  0.30294046]]\n",
      "    --  Umeyama Translation: \n",
      " [[ 1.3542229 ]\n",
      " [-1.88143672]\n",
      " [ 0.71802461]]\n",
      "Skipping treehill as required files are missing.\n",
      "    --  Umeyama Scale:  0.5685251628410306\n",
      "    --  Umeyama Rotation: \n",
      " [[-0.84703425  0.1114501  -0.51972286]\n",
      " [-0.45210337  0.36313373  0.81470021]\n",
      " [ 0.27952732  0.92504744 -0.25719974]]\n",
      "    --  Umeyama Translation: \n",
      " [[-1.1589561 ]\n",
      " [ 0.83238572]\n",
      " [ 0.73164876]]\n",
      "    --  Umeyama Scale:  0.31234358905306103\n",
      "    --  Umeyama Rotation: \n",
      " [[-0.7627363  -0.39015712 -0.51576231]\n",
      " [-0.37982751 -0.37521295  0.84554498]\n",
      " [-0.52341609  0.84082856  0.13799613]]\n",
      "    --  Umeyama Translation: \n",
      " [[-0.53731587]\n",
      " [ 0.09935859]\n",
      " [-2.92350555]]\n",
      "    --  Umeyama Scale:  0.640590123341812\n",
      "    --  Umeyama Rotation: \n",
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25057826  0.90359273  0.34746325]\n",
      " [-0.39124894  0.42281895 -0.81740345]\n",
      " [-0.88551387  0.06887891  0.45947893]]\n",
      "    --  Umeyama Translation: \n",
      " [[-0.6953709 ]\n",
      " [-2.4318474 ]\n",
      " [-3.26104026]]\n",
      "    --  Umeyama Scale:  0.5508541530367199\n",
      "    --  Umeyama Rotation: \n",
      " [[-0.04744721 -0.25753836  0.96510246]\n",
      " [ 0.96990909 -0.24286435 -0.01712502]\n",
      " [ 0.23879933  0.93524911  0.26131203]]\n",
      "    --  Umeyama Translation: \n",
      " [[3.66612873]\n",
      " [0.01451746]\n",
      " [6.28388313]]\n",
      "    --  Umeyama Scale:  0.5122194321156387\n",
      "    --  Umeyama Rotation: \n",
      " [[ 0.69930871  0.68414105 -0.20716747]\n",
      " [ 0.65734963 -0.50163019  0.56236876]\n",
      " [ 0.28081809 -0.52945083 -0.80051422]]\n",
      "    --  Umeyama Translation: \n",
      " [[1.91606598]\n",
      " [0.06981268]\n",
      " [1.88802055]]\n"
     ]
    }
   ],
   "source": [
    "import utils.colmap as colmap_utils\n",
    "from utils.metric_torch import camera_to_rel_deg, calculate_auc_np\n",
    "\n",
    "# Get image paths and preprocess them\n",
    "data_dir = \"../data/MipNeRF360\"\n",
    "\n",
    "# wald through folders under data_dir and calculate average performance\n",
    "rot_error = []\n",
    "translation_error = []\n",
    "auc_30 = []\n",
    "\n",
    "for subdir in os.listdir(data_dir):\n",
    "\n",
    "    if not os.path.isdir(os.path.join(data_dir, subdir)):\n",
    "        continue\n",
    "\n",
    "    dust_dir = os.path.join(data_dir, subdir, \"mast3r\")\n",
    "\n",
    "    if not os.path.exists(os.path.join(dust_dir, 'camera_poses.npy')) or \\\n",
    "         not os.path.exists(os.path.join(data_dir, subdir, 'pose_gt_train.npy')):\n",
    "        print(f\"Skipping {subdir} as required files are missing.\")\n",
    "        continue\n",
    "    \n",
    "    camtoworlds_train = np.load(os.path.join(dust_dir, 'camera_poses.npy'))\n",
    "    camtoworlds_train_gt = np.load(os.path.join(data_dir, subdir, 'pose_gt_train.npy'))\n",
    "\n",
    "    mast3r_se3 = torch.tensor(np.linalg.inv(camtoworlds_train), device=device)\n",
    "    mast3r_gt_se3 = torch.tensor(np.linalg.inv(camtoworlds_train_gt), device=device)\n",
    "\n",
    "    mast3r_se3[:, 3, :3] = mast3r_se3[:, :3, 3]\n",
    "    mast3r_se3[:, :3, 3] = 0.0\n",
    "    mast3r_gt_se3[:, 3, :3] = mast3r_gt_se3[:, :3, 3]\n",
    "    mast3r_gt_se3[:, :3, 3] = 0.0\n",
    "\n",
    "    # add alignment\n",
    "    camera_centers_mast3r_gt = - (mast3r_gt_se3[:, :3, :3].cpu().numpy().transpose(0, 2, 1) @ mast3r_gt_se3[:, 3, :3][..., None].cpu().numpy()).squeeze(-1)\n",
    "    camera_centers_mast3r_pred = - (mast3r_se3[:, :3, :3].cpu().numpy().transpose(0, 2, 1) @ mast3r_se3[:, 3, :3][..., None].cpu().numpy()).squeeze(-1)\n",
    "    c, R, t = umeyama(camera_centers_mast3r_gt.T, camera_centers_mast3r_pred.T)\n",
    "    camera_centers_mast3r_gt_aligned = (c * (R @ camera_centers_mast3r_gt.T) + t).T\n",
    "    print(\"    --  Umeyama Scale: \", c)\n",
    "    print(\"    --  Umeyama Rotation: \\n\", R)\n",
    "    print(\"    --  Umeyama Translation: \\n\", t)\n",
    "\n",
    "    ext_transform = np.eye(4)\n",
    "    ext_transform[:3, :3] = R\n",
    "    ext_transform[:3, 3:] = t\n",
    "    ext_transform = np.linalg.inv(ext_transform)\n",
    "\n",
    "    mast3r_gt_aligned = np.zeros((camtoworlds_train.shape[0], 4, 4))\n",
    "    mast3r_gt_aligned[:, :3, :3] = mast3r_gt_se3[:, :3, :3].cpu().numpy()\n",
    "    mast3r_gt_aligned[:, :3, 3] = mast3r_gt_se3[:, 3, :3].cpu().numpy() * c\n",
    "    mast3r_gt_aligned[:, 3, 3] = 1.0\n",
    "    mast3r_gt_aligned = np.einsum('bmn,bnk->bmk', mast3r_gt_aligned, ext_transform[None])\n",
    "\n",
    "    mast3r_gt_se3_aligned = torch.eye(4, device=device).unsqueeze(0).repeat(camtoworlds_train.shape[0], 1, 1)\n",
    "    mast3r_gt_se3_aligned[:, :3, :3] = torch.tensor(mast3r_gt_aligned[:, :3, :3], device=device)\n",
    "    mast3r_gt_se3_aligned[:, 3, :3] = torch.tensor(mast3r_gt_aligned[:, :3, 3], device=device)\n",
    "\n",
    "    rel_rangle_deg, rel_tangle_deg = camera_to_rel_deg(mast3r_se3, mast3r_gt_se3_aligned, device, 4)\n",
    "\n",
    "    rError = rel_rangle_deg.cpu().numpy()\n",
    "    tError = rel_tangle_deg.cpu().numpy()\n",
    "\n",
    "    Auc_30 = calculate_auc_np(rError, tError, max_threshold=30)\n",
    "    \n",
    "    rot_error.append(rError.mean())\n",
    "    translation_error.append(tError.mean())\n",
    "    auc_30.append(Auc_30)\n",
    "\n",
    "print(\"Average Rotation Error: \", np.mean(rot_error))\n",
    "print(\"Average Translation Error: \", np.mean(translation_error))\n",
    "print(\"Average AUC@30: \", np.mean(auc_30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vggt_310_bkup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
