{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import io\n",
    "import evo\n",
    "import evo.main_ape as main_ape\n",
    "import evo.main_rpe as main_rpe\n",
    "\n",
    "from tqdm import tqdm\n",
    "from evo.core.metrics import PoseRelation, Unit\n",
    "from evo.core.trajectory import PoseTrajectory3D\n",
    "from evo.core import lie_algebra\n",
    "from evo.tools.plot import PlotMode\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.transform import Rotation\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images_ratio\n",
    "from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "from vggt.utils.helper import create_pixel_coordinate_grid, randomly_limit_trues\n",
    "from vggt.dependency.track_predict import predict_tracks\n",
    "from vggt.dependency.np_to_pycolmap import batch_np_matrix_to_pycolmap, batch_np_matrix_to_pycolmap_wo_track\n",
    "\n",
    "from utils.umeyama import umeyama\n",
    "from utils.cam_viz import create_interactive_camera_animation\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping flowers as required files are missing.\n",
      "Skipping treehill as required files are missing.\n",
      "Average Rotation Error:  0.6430236782346453\n",
      "Average Translation Error:  0.7762274060930524\n",
      "Average AUC@30:  0.9849837238838565\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import utils.colmap as colmap_utils\n",
    "from utils.metric_torch import camera_to_rel_deg, calculate_auc_np\n",
    "\n",
    "# Get image paths and preprocess them\n",
    "data_dir = \"../data/MipNeRF360\"\n",
    "\n",
    "# wald through folders under data_dir and calculate average performance\n",
    "rot_error = []\n",
    "translation_error = []\n",
    "auc_30 = []\n",
    "\n",
    "for subdir in os.listdir(data_dir):\n",
    "\n",
    "    if not os.path.isdir(os.path.join(data_dir, subdir)):\n",
    "        continue\n",
    "\n",
    "    dust_dir = os.path.join(data_dir, subdir, \"mast3r\")\n",
    "\n",
    "    if not os.path.exists(os.path.join(dust_dir, 'camera_poses.npy')) or \\\n",
    "         not os.path.exists(os.path.join(data_dir, subdir, 'pose_gt_train.npy')):\n",
    "        print(f\"Skipping {subdir} as required files are missing.\")\n",
    "        continue\n",
    "    \n",
    "    camtoworlds_train = np.load(os.path.join(dust_dir, 'camera_poses.npy'))\n",
    "    camtoworlds_train_gt = np.load(os.path.join(data_dir, subdir, 'pose_gt_train.npy'))\n",
    "\n",
    "    mast3r_se3 = torch.tensor(np.linalg.inv(camtoworlds_train), device=device)\n",
    "    mast3r_gt_se3 = torch.tensor(np.linalg.inv(camtoworlds_train_gt), device=device)\n",
    "\n",
    "    mast3r_se3[:, 3, :3] = mast3r_se3[:, :3, 3]\n",
    "    mast3r_se3[:, :3, 3] = 0.0\n",
    "    mast3r_gt_se3[:, 3, :3] = mast3r_gt_se3[:, :3, 3]\n",
    "    mast3r_gt_se3[:, :3, 3] = 0.0\n",
    "\n",
    "    # add alignment\n",
    "    camera_centers_mast3r_gt = - (mast3r_gt_se3[:, :3, :3].cpu().numpy().transpose(0, 2, 1) @ mast3r_gt_se3[:, 3, :3][..., None].cpu().numpy()).squeeze(-1)\n",
    "    camera_centers_mast3r_pred = - (mast3r_se3[:, :3, :3].cpu().numpy().transpose(0, 2, 1) @ mast3r_se3[:, 3, :3][..., None].cpu().numpy()).squeeze(-1)\n",
    "    c, R, t = umeyama(camera_centers_mast3r_gt.T, camera_centers_mast3r_pred.T)\n",
    "    camera_centers_mast3r_gt_aligned = (c * (R @ camera_centers_mast3r_gt.T) + t).T\n",
    "\n",
    "    ext_transform = np.eye(4)\n",
    "    ext_transform[:3, :3] = R\n",
    "    ext_transform[:3, 3:] = t\n",
    "    ext_transform = np.linalg.inv(ext_transform)\n",
    "\n",
    "    mast3r_gt_aligned = np.zeros((camtoworlds_train.shape[0], 4, 4))\n",
    "    mast3r_gt_aligned[:, :3, :3] = mast3r_gt_se3[:, :3, :3].cpu().numpy()\n",
    "    mast3r_gt_aligned[:, :3, 3] = mast3r_gt_se3[:, 3, :3].cpu().numpy() * c\n",
    "    mast3r_gt_aligned[:, 3, 3] = 1.0\n",
    "    mast3r_gt_aligned = np.einsum('bmn,bnk->bmk', mast3r_gt_aligned, ext_transform[None])\n",
    "\n",
    "    mast3r_gt_se3_aligned = torch.eye(4, device=device).unsqueeze(0).repeat(camtoworlds_train.shape[0], 1, 1)\n",
    "    mast3r_gt_se3_aligned[:, :3, :3] = torch.tensor(mast3r_gt_aligned[:, :3, :3], device=device)\n",
    "    mast3r_gt_se3_aligned[:, 3, :3] = torch.tensor(mast3r_gt_aligned[:, :3, 3], device=device)\n",
    "\n",
    "    rel_rangle_deg, rel_tangle_deg = camera_to_rel_deg(mast3r_se3, mast3r_gt_se3_aligned, device, 4);\n",
    "\n",
    "    rError = rel_rangle_deg.cpu().numpy()\n",
    "    tError = rel_tangle_deg.cpu().numpy()\n",
    "\n",
    "    Auc_30 = calculate_auc_np(rError, tError, max_threshold=30);\n",
    "    \n",
    "    rot_error.append(rError.mean().item())\n",
    "    translation_error.append(tError.mean().item())\n",
    "    auc_30.append(Auc_30)\n",
    "\n",
    "print(\"Average Rotation Error: \", np.mean(rot_error))\n",
    "print(\"Average Translation Error: \", np.mean(translation_error))\n",
    "print(\"Average AUC@30: \", np.mean(auc_30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vggt_310_bkup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
