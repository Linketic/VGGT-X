{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "import roma\n",
    "import kornia\n",
    "import utils.colmap as colmap_utils\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images_ratio, load_and_preprocess_images_square\n",
    "from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "\n",
    "from utils.umeyama import umeyama\n",
    "from utils.metric_torch import evaluate_auc, evaluate_pcd\n",
    "\n",
    "torch._dynamo.config.accumulated_cache_size_limit = 512\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_VGGT(images, device, dtype):\n",
    "    # images: [B, 3, H, W]\n",
    "\n",
    "    # Run VGGT for camera and depth estimation\n",
    "    model = VGGT()\n",
    "    _URL = \"https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt\"\n",
    "    model.load_state_dict(torch.hub.load_state_dict_from_url(_URL))\n",
    "    model.eval()\n",
    "    model = model.to(device).to(dtype)\n",
    "    print(f\"Model loaded\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images.to(device, dtype), verbose=True)\n",
    "        extrinsic, intrinsic = pose_encoding_to_extri_intri(predictions['pose_enc'], images.shape[-2:])\n",
    "        extrinsic = extrinsic.squeeze(0).cpu().numpy()\n",
    "        intrinsic = intrinsic.squeeze(0).cpu().numpy()\n",
    "        depth_map = predictions['depth'].squeeze(0).cpu().numpy()\n",
    "        depth_conf = predictions['depth_conf'].squeeze(0).cpu().numpy()\n",
    "    \n",
    "    return extrinsic, intrinsic, depth_map, depth_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scene: bicycle\n",
      "Intrinsic Difference 0.018999718823743232\n",
      "Processing scene: bonsai\n",
      "Intrinsic Difference 0.02563519124160258\n",
      "Processing scene: counter\n",
      "Intrinsic Difference 0.027854708642436523\n",
      "Processing scene: flowers\n",
      "Intrinsic Difference 0.07321011271751116\n",
      "Processing scene: garden\n",
      "Intrinsic Difference 0.003909443165705178\n",
      "Processing scene: kitchen\n",
      "Intrinsic Difference 0.032984788461085386\n",
      "Processing scene: room\n",
      "Intrinsic Difference 0.05987657710889783\n",
      "Processing scene: stump\n",
      "Intrinsic Difference 0.03201081516783678\n",
      "Processing scene: treehill\n",
      "Intrinsic Difference 0.05678942490718071\n"
     ]
    }
   ],
   "source": [
    "# Get image paths and preprocess them\n",
    "data_dir_gt = \"../data/MipNeRF360\"\n",
    "data_dir_pred = \"../data/MipNeRF360_vggt_opt_lr_5_avg_scale_500k_rand_order\"\n",
    "\n",
    "scenes = sorted(os.listdir(data_dir_gt))\n",
    "for scene in scenes:\n",
    "    if os.path.isdir(os.path.join(data_dir_gt, scene)):\n",
    "        print(f\"Processing scene: {scene}\")\n",
    "        sparse_dir_gt = os.path.join(data_dir_gt, scene, \"sparse\", \"0\")\n",
    "        images_dir = os.path.join(data_dir_gt, scene, \"images\")\n",
    "\n",
    "        cameras_gt = colmap_utils.read_cameras_binary(os.path.join(sparse_dir_gt, \"cameras.bin\"))\n",
    "        images_gt = colmap_utils.read_images_binary(os.path.join(sparse_dir_gt, \"images.bin\"))\n",
    "        pcd_gt = colmap_utils.read_points3D_binary(os.path.join(sparse_dir_gt, \"points3D.bin\"))\n",
    "\n",
    "        sparse_dir_pred = os.path.join(data_dir_pred, scene, \"sparse\", \"0\")\n",
    "        cameras_pred = colmap_utils.read_cameras_binary(os.path.join(sparse_dir_pred, \"cameras.bin\"))\n",
    "        images_pred = colmap_utils.read_images_binary(os.path.join(sparse_dir_pred, \"images.bin\"))\n",
    "        pcd_pred = colmap_utils.read_points3D_binary(os.path.join(sparse_dir_pred, \"points3D.bin\"))\n",
    "\n",
    "        # print(f\"GT's intrinsics: {cameras_gt[1].params}\")\n",
    "        # print(f\"Pred's intrinsics: {cameras_pred[1].params}\")\n",
    "        diff = cameras_gt[1].params - cameras_pred[1].params\n",
    "        print(\"Intrinsic Difference\", np.linalg.norm(diff[:2] / cameras_gt[1].params[2:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vggt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
