{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import io\n",
    "import evo\n",
    "import evo.main_ape as main_ape\n",
    "import evo.main_rpe as main_rpe\n",
    "\n",
    "from tqdm import tqdm\n",
    "from evo.core.metrics import PoseRelation, Unit\n",
    "from evo.core.trajectory import PoseTrajectory3D\n",
    "from evo.core import lie_algebra\n",
    "from evo.tools.plot import PlotMode\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.transform import Rotation\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images_ratio, load_and_preprocess_images_square\n",
    "from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "from vggt.utils.helper import create_pixel_coordinate_grid, randomly_limit_trues\n",
    "from vggt.dependency.track_predict import predict_tracks\n",
    "from vggt.dependency.np_to_pycolmap import batch_np_matrix_to_pycolmap, batch_np_matrix_to_pycolmap_wo_track\n",
    "\n",
    "from utils.umeyama import umeyama\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_VGGT(model, images, dtype, resolution=518, track_feat=False):\n",
    "    # images: [B, 3, H, W]\n",
    "\n",
    "    assert len(images.shape) == 4\n",
    "    assert images.shape[1] == 3\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    images = images.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(dtype=dtype):\n",
    "            images = images[None]  # add batch dimension\n",
    "            valid_layers = model.depth_head.intermediate_layer_idx\n",
    "            if valid_layers[-1] != model.aggregator.aa_block_num - 1:\n",
    "                valid_layers.append(model.aggregator.aa_block_num - 1)\n",
    "            aggregated_tokens_list, ps_idx = model.aggregator(images, valid_layers)\n",
    "            aggregated_tokens_list = [tokens.to(device) if tokens is not None else None for tokens in aggregated_tokens_list]\n",
    "\n",
    "        # Predict Cameras\n",
    "        pose_enc = model.camera_head(aggregated_tokens_list)[-1]\n",
    "        # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)\n",
    "        extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])\n",
    "        # Predict Depth Maps\n",
    "        depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)\n",
    "\n",
    "        extrinsic = extrinsic.squeeze(0).cpu().numpy()\n",
    "        intrinsic = intrinsic.squeeze(0).cpu().numpy()\n",
    "        depth_map = depth_map.squeeze(0).cpu().numpy()\n",
    "        depth_conf = depth_conf.squeeze(0).cpu().numpy()\n",
    "\n",
    "        track_feature_maps = None if not track_feat else model.track_head.feature_extractor(aggregated_tokens_list, images, ps_idx)\n",
    "        \n",
    "    return extrinsic, intrinsic, depth_map, depth_conf, track_feature_maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGGT(\n",
       "  (aggregator): Aggregator(\n",
       "    (patch_embed): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x NestedTensorBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MemEffAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (rope): RotaryPositionEmbedding2D()\n",
       "    (frame_blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): RotaryPositionEmbedding2D()\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (global_blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): RotaryPositionEmbedding2D()\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (camera_head): CameraHead(\n",
       "    (trunk): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (token_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (trunk_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (embed_pose): Linear(in_features=9, out_features=2048, bias=True)\n",
       "    (poseLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "    )\n",
       "    (adaln_norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=False)\n",
       "    (pose_branch): Mlp(\n",
       "      (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (fc2): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (drop): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (point_head): DPTHead(\n",
       "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (depth_head): DPTHead(\n",
       "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (track_head): TrackHead(\n",
       "    (feature_extractor): DPTHead(\n",
       "      (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (projects): ModuleList(\n",
       "        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (resize_layers): ModuleList(\n",
       "        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (2): Identity()\n",
       "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (scratch): Module(\n",
       "        (layer1_rn): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer2_rn): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer3_rn): Conv2d(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer4_rn): Conv2d(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (refinenet1): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet2): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet3): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet4): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (output_conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (tracker): BaseTrackerPredictor(\n",
       "      (corr_mlp): Mlp(\n",
       "        (fc1): Linear(in_features=567, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (updateformer): EfficientUpdateFormer(\n",
       "        (input_norm): LayerNorm((388,), eps=1e-05, elementwise_affine=True)\n",
       "        (input_transform): Linear(in_features=388, out_features=384, bias=True)\n",
       "        (output_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (flow_head): Linear(in_features=384, out_features=130, bias=True)\n",
       "        (time_blocks): ModuleList(\n",
       "          (0-5): 6 x AttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_virtual_blocks): ModuleList(\n",
       "          (0-5): 6 x AttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_point2virtual_blocks): ModuleList(\n",
       "          (0-5): 6 x CrossAttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_context): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_virtual2point_blocks): ModuleList(\n",
       "          (0-5): 6 x CrossAttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_context): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fmap_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffeat_norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      (ffeat_updater): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (vis_predictor): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "      (conf_predictor): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model and load the pretrained weights.\n",
    "# This will automatically download the model weights the first time it's run, which may take a while.\n",
    "model = VGGT.from_pretrained(\"facebook/VGGT-1B\").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.colmap as colmap_utils\n",
    "\n",
    "# Get image paths and preprocess them\n",
    "sparse_dir_gt = \"../data/MipNeRF360/treehill/sparse/0\"\n",
    "sparse_dir_pred = \"../data/MipNeRF360_vggt/treehill/sparse/0\"\n",
    "images_dir = \"../data/MipNeRF360/treehill/images\"\n",
    "\n",
    "cameras_gt = colmap_utils.read_cameras_binary(os.path.join(sparse_dir_gt, \"cameras.bin\"))\n",
    "images_gt = colmap_utils.read_images_binary(os.path.join(sparse_dir_gt, \"images.bin\"))\n",
    "pcd_gt = colmap_utils.read_points3D_binary(os.path.join(sparse_dir_gt, \"points3D.bin\"))\n",
    "# images_gt = dict(sorted(images_gt.items(), key=lambda item: item[0]))\n",
    "\n",
    "# cameras_pred = colmap_utils.read_cameras_binary(os.path.join(sparse_dir_pred, \"cameras.bin\"))\n",
    "# images_pred = colmap_utils.read_images_binary(os.path.join(sparse_dir_pred, \"images.bin\"))\n",
    "# pcd_pred = colmap_utils.read_points3D_binary(os.path.join(sparse_dir_pred, \"points3D.bin\"))\n",
    "# images_pred = dict(sorted(images_pred.items(), key=lambda item: item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_gt_updated = {id: images_gt[id] for id in list(images_gt.keys())}\n",
    "image_path_list = [os.path.join(images_dir, images_gt_updated[id].name) for id in images_gt_updated.keys()]\n",
    "base_image_path_list = [os.path.basename(path) for path in image_path_list]\n",
    "total_frame_num = len(image_path_list)\n",
    "\n",
    "vggt_fixed_resolution = 518\n",
    "images, original_coords = load_and_preprocess_images_ratio(image_path_list, vggt_fixed_resolution)\n",
    "\n",
    "# plot images\n",
    "# plt.figure(figsize=(16, 10))\n",
    "# for i, img in enumerate(images):\n",
    "#     plt.subplot(5, 5, i + 1)\n",
    "#     plt.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "#     plt.title(base_image_path_list[i].split('_Zenmuse')[0])\n",
    "#     plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run VGGT to estimate camera and depth\n",
    "# Run with 518x518 images\n",
    "extrinsic, intrinsic, depth_map, depth_conf, track_feats = run_VGGT(model, images, dtype, vggt_fixed_resolution, track_feat=True)\n",
    "points_3d = unproject_depth_map_to_point_map(depth_map, extrinsic, intrinsic)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3872/693412443.py:15: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 517.5, 335.5, -0.5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_threshold = 2\n",
    "num_track_pts = 50\n",
    "mask = depth_conf > conf_threshold\n",
    "\n",
    "# idx = random.randint(0, total_frame_num - 1)\n",
    "idx = 0\n",
    "image_masked = images[idx].cpu().numpy().transpose(1, 2, 0) * mask[idx][:, :, None]\n",
    "image_org = images[idx].cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_org)\n",
    "plt.title(f\"Original Image of {idx}\")\n",
    "plt.tight_layout()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_masked)\n",
    "plt.title(\"Masked Image with Depth Confidence > {}\".format(conf_threshold))\n",
    "# pick up num_track_pts points with highest depth confidence and mark them as red x on the image\n",
    "# mark_points = np.argsort(depth_conf[idx].flatten())[-num_track_pts:]\n",
    "# randomly select num_track_pts points with highest depth confidence and mark them as red x on the image\n",
    "valid_index = np.where(mask[idx].flatten())[0].tolist()\n",
    "mark_points = random.sample(valid_index, num_track_pts)\n",
    "for point in mark_points:\n",
    "    y, x = divmod(point, depth_conf[idx].shape[1])\n",
    "    plt.scatter(x, y, color='red', s=10, marker='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYZE_EXAMPLE=False\n",
    "\n",
    "if ANALYZE_EXAMPLE:\n",
    "    # Predict Tracks\n",
    "    # choose your own points to track, with shape (N, 2) for one scene\n",
    "    from vggt.utils.visual_track import visualize_tracks_on_images\n",
    "\n",
    "    conf_threshold = 2\n",
    "    num_track_pts = 50\n",
    "    mask = depth_conf > conf_threshold\n",
    "\n",
    "    start_idx = 0\n",
    "    corr_mask = np.zeros(depth_conf.shape[0], dtype=bool)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "        aggregated_tokens_list, ps_idx = run_VGGT(model, images, dtype, vggt_fixed_resolution, feat_only=True)\n",
    "\n",
    "    while any(corr_mask) is False:\n",
    "\n",
    "        query_points_list = []\n",
    "        valid_index = np.where(mask[start_idx].flatten())[0].tolist()\n",
    "        mark_points = random.sample(valid_index, num_track_pts)\n",
    "        for point in mark_points:\n",
    "            y, x = divmod(point, depth_conf[start_idx].shape[1])\n",
    "            query_points_list.append([x, y])\n",
    "        query_points = torch.FloatTensor(query_points_list).to(device)\n",
    "\n",
    "        # reorder the image, make the start_idx image the first one\n",
    "        reordered_idx = list(range(start_idx, total_frame_num)) + list(range(0, start_idx))\n",
    "        reordered_aggregated_tokens_list = [aggregated_tokens_list[i] if aggregated_tokens_list[i] is None \\\n",
    "                                            else aggregated_tokens_list[i][:, reordered_idx] for i in range(len(aggregated_tokens_list))]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            track_list, vis_score, conf_score = model.track_head(reordered_aggregated_tokens_list, images[None, reordered_idx], \n",
    "                                                                ps_idx, query_points=query_points[None])\n",
    "            valid_track_score_mask = (conf_score > 0.2) & (vis_score > 0.2)\n",
    "            valid_track_num = valid_track_score_mask.sum(dim=-1)\n",
    "            valid_track_num_mask = valid_track_num > num_track_pts // 2\n",
    "            valid_frame_idx = torch.where(valid_track_num_mask[0])[0].tolist()\n",
    "\n",
    "    visualize_tracks_on_images(images, track_list[-1], (conf_score>0.2) & (vis_score>0.2), out_dir=\"track_visuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of query points: 100, start index: 0, 141 frames are rest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0000/tracks_grid.png\n",
      "[INFO] Saved 72 individual frames to track_visuals/0000_start_0000/frame_*.png\n",
      "Number of query points: 100, start index: 9, 69 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0009/tracks_grid.png\n",
      "[INFO] Saved 36 individual frames to track_visuals/0000_start_0009/frame_*.png\n",
      "Number of query points: 100, start index: 14, 60 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0014/tracks_grid.png\n",
      "[INFO] Saved 3 individual frames to track_visuals/0000_start_0014/frame_*.png\n",
      "Number of query points: 100, start index: 15, 58 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0015/tracks_grid.png\n",
      "[INFO] Saved 15 individual frames to track_visuals/0000_start_0015/frame_*.png\n",
      "Number of query points: 100, start index: 18, 49 frames are rest\n",
      "No valid tracks found for frame 18, skipping...\n",
      "Number of query points: 100, start index: 19, 49 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0019/tracks_grid.png\n",
      "[INFO] Saved 9 individual frames to track_visuals/0000_start_0019/frame_*.png\n",
      "Number of query points: 100, start index: 23, 40 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0023/tracks_grid.png\n",
      "[INFO] Saved 44 individual frames to track_visuals/0000_start_0023/frame_*.png\n",
      "Number of query points: 100, start index: 31, 30 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0031/tracks_grid.png\n",
      "[INFO] Saved 77 individual frames to track_visuals/0000_start_0031/frame_*.png\n",
      "Number of query points: 100, start index: 42, 25 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0042/tracks_grid.png\n",
      "[INFO] Saved 5 individual frames to track_visuals/0000_start_0042/frame_*.png\n",
      "Number of query points: 100, start index: 66, 20 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0066/tracks_grid.png\n",
      "[INFO] Saved 10 individual frames to track_visuals/0000_start_0066/frame_*.png\n",
      "Number of query points: 100, start index: 70, 19 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0070/tracks_grid.png\n",
      "[INFO] Saved 6 individual frames to track_visuals/0000_start_0070/frame_*.png\n",
      "Number of query points: 100, start index: 72, 16 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0072/tracks_grid.png\n",
      "[INFO] Saved 8 individual frames to track_visuals/0000_start_0072/frame_*.png\n",
      "Number of query points: 100, start index: 91, 13 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0091/tracks_grid.png\n",
      "[INFO] Saved 17 individual frames to track_visuals/0000_start_0091/frame_*.png\n",
      "Number of query points: 100, start index: 95, 11 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0095/tracks_grid.png\n",
      "[INFO] Saved 3 individual frames to track_visuals/0000_start_0095/frame_*.png\n",
      "Number of query points: 100, start index: 97, 10 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0097/tracks_grid.png\n",
      "[INFO] Saved 14 individual frames to track_visuals/0000_start_0097/frame_*.png\n",
      "Number of query points: 100, start index: 113, 6 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0113/tracks_grid.png\n",
      "[INFO] Saved 2 individual frames to track_visuals/0000_start_0113/frame_*.png\n",
      "Number of query points: 100, start index: 115, 5 frames are rest\n",
      "No valid tracks found for frame 115, skipping...\n",
      "Number of query points: 100, start index: 118, 5 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0118/tracks_grid.png\n",
      "[INFO] Saved 6 individual frames to track_visuals/0000_start_0118/frame_*.png\n",
      "Number of query points: 100, start index: 115, 3 frames are rest\n",
      "No valid tracks found for frame 115, skipping...\n",
      "Number of query points: 100, start index: 119, 3 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0119/tracks_grid.png\n",
      "[INFO] Saved 13 individual frames to track_visuals/0000_start_0119/frame_*.png\n",
      "Number of query points: 100, start index: 115, 2 frames are rest\n",
      "No valid tracks found for frame 115, skipping...\n",
      "Number of query points: 100, start index: 135, 2 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0135/tracks_grid.png\n",
      "[INFO] Saved 17 individual frames to track_visuals/0000_start_0135/frame_*.png\n",
      "Number of query points: 100, start index: 115, 1 frames are rest\n",
      "[INFO] Saved color-by-XY track visualization grid -> track_visuals/0000_start_0115/tracks_grid.png\n",
      "[INFO] Saved 2 individual frames to track_visuals/0000_start_0115/frame_*.png\n",
      "Total 359 correspondence pairs are predicted.\n",
      "Total 0 frames are find no correspondence.\n"
     ]
    }
   ],
   "source": [
    "# Predict Tracks\n",
    "# choose your own points to track, with shape (N, 2) for one scene\n",
    "from vggt.utils.visual_track import visualize_tracks_on_images\n",
    "\n",
    "# delete the track_visuals directory if it exists\n",
    "if os.path.exists(\"track_visuals\"):\n",
    "    import shutil\n",
    "    shutil.rmtree(\"track_visuals\")\n",
    "\n",
    "conf_threshold = 2.0\n",
    "max_num_track_pts = 100\n",
    "query_frame_num = total_frame_num // 2\n",
    "mask = depth_conf > conf_threshold\n",
    "\n",
    "corr_mask = np.zeros(depth_conf.shape[0], dtype=bool)\n",
    "rest_frame_idx = np.where(~corr_mask)[0].tolist()\n",
    "\n",
    "tracks_list = []\n",
    "vis_scores_list = []\n",
    "conf_scores_list = []\n",
    "frame_idx_list = []\n",
    "tgt_idx_list = []\n",
    "iteration = 0\n",
    "\n",
    "while all(corr_mask) is False:\n",
    "\n",
    "    query_points_list = []\n",
    "    if len(rest_frame_idx) > 0:\n",
    "        start_idx = rest_frame_idx.pop(0)\n",
    "    else:\n",
    "        break\n",
    "    valid_index = np.where(mask[start_idx].flatten())[0].tolist()\n",
    "    # num_track_pts = min(max_num_track_pts, len(valid_index))\n",
    "    num_track_pts = max_num_track_pts\n",
    "    mark_points = np.random.choice(valid_index, num_track_pts, replace=True).tolist()\n",
    "    for point in mark_points:\n",
    "        y, x = divmod(point, depth_conf[start_idx].shape[1])\n",
    "        query_points_list.append([x, y])\n",
    "    \n",
    "    if len(query_points_list) == 0:\n",
    "        print(f\"No valid points found for frame {start_idx}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    query_points = torch.FloatTensor(query_points_list).to(device)\n",
    "\n",
    "    # reorder the image, make the start_idx image the first one\n",
    "    reordered_idx = list(range(start_idx, total_frame_num)) + list(range(0, start_idx))\n",
    "    # reordered_idx = [start_idx] + random.sample(reordered_idx[1:], query_frame_num - 1)\n",
    "    with torch.no_grad():\n",
    "        print(f\"Number of query points: {len(query_points_list)}, start index: {start_idx}, {sum(~corr_mask)} frames are rest\")\n",
    "        track_list, vis_score, conf_score = model.track_head.tracker(query_points=query_points[None], fmaps=track_feats[:, reordered_idx], iters=model.track_head.iters)\n",
    "        valid_track_score_mask = (conf_score > 0.2) & (vis_score > 0.2)\n",
    "        valid_track_num = valid_track_score_mask.sum(dim=-1)\n",
    "        valid_track_num_mask = valid_track_num > num_track_pts // 4\n",
    "        valid_idx = torch.where(valid_track_num_mask[0])[0].tolist()\n",
    "\n",
    "        if len(valid_idx) <= 1:\n",
    "            print(f\"No valid tracks found for frame {start_idx}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        tracks_list.append(track_list[-1][0, valid_idx].cpu().numpy())\n",
    "        vis_scores_list.append(vis_score[0, valid_idx].cpu().numpy())\n",
    "        conf_scores_list.append(conf_score[0, valid_idx].cpu().numpy())\n",
    "        tgt_idx_list += [len(frame_idx_list)] * len(valid_idx)\n",
    "        valid_frame_idx = [reordered_idx[i] for i in valid_idx]\n",
    "        frame_idx_list += valid_frame_idx\n",
    "        \n",
    "    corr_mask[valid_frame_idx] = True\n",
    "    rest_frame_idx = np.where(~corr_mask)[0].tolist()\n",
    "    \n",
    "    visualize_tracks_on_images(images[reordered_idx][valid_idx], track_list[-1][:, valid_idx], valid_track_score_mask[:, valid_idx], out_dir=f\"track_visuals/{iteration:04d}_start_{start_idx:04d}\")\n",
    "\n",
    "print(f\"Total {len(frame_idx_list)} correspondence pairs are predicted.\")\n",
    "print(f\"Total {sum(~corr_mask)} frames are find no correspondence.\")\n",
    "# visualize_tracks_on_images(images, track_list[-1], (conf_score>0.2) & (vis_score>0.2), out_dir=\"track_visuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3872/3520347096.py:84: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "100%|██████████| 131/131 [00:00<00:00, 3854.22it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 1794.78it/s]\n",
      "/tmp/ipykernel_3872/3520347096.py:117: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "import kornia\n",
    "from scipy.ndimage import map_coordinates\n",
    "\n",
    "image_names = np.array([base_image_path_list[i] for i in frame_idx_list])\n",
    "tracks = np.concatenate(tracks_list, axis=0)\n",
    "vis_scores = np.concatenate(vis_scores_list, axis=0)\n",
    "conf_scores = np.concatenate(conf_scores_list, axis=0)\n",
    "target_indexes = np.array(tgt_idx_list)\n",
    "frame_indexes = np.array(frame_idx_list)\n",
    "\n",
    "frame_indexes = frame_indexes[:, None].repeat(tracks.shape[1], axis=1)\n",
    "frame_names = image_names[:, None].repeat(tracks.shape[1], axis=1)\n",
    "valid_track_score_mask = (conf_scores > 0.2) & (vis_scores > 0.2)\n",
    "\n",
    "corr_points_i = tracks[valid_track_score_mask]\n",
    "corr_points_j = tracks[target_indexes][valid_track_score_mask]\n",
    "\n",
    "image_names_i = frame_names[valid_track_score_mask]\n",
    "image_names_j = frame_names[target_indexes][valid_track_score_mask]\n",
    "\n",
    "frame_indexes_i = frame_indexes[valid_track_score_mask]\n",
    "frame_indexes_j = frame_indexes[target_indexes][valid_track_score_mask]\n",
    "\n",
    "vis_scores_i = vis_scores[valid_track_score_mask]\n",
    "vis_scores_j = vis_scores[target_indexes][valid_track_score_mask]\n",
    "\n",
    "conf_scores_i = conf_scores[valid_track_score_mask]\n",
    "conf_scores_j = conf_scores[target_indexes][valid_track_score_mask]\n",
    "\n",
    "same_pt_mask = (image_names_i == image_names_j)\n",
    "corr_points_i = corr_points_i[~same_pt_mask]\n",
    "corr_points_j = corr_points_j[~same_pt_mask]\n",
    "image_names_i = image_names_i[~same_pt_mask]\n",
    "image_names_j = image_names_j[~same_pt_mask]\n",
    "frame_indexes_i = frame_indexes_i[~same_pt_mask]\n",
    "frame_indexes_j = frame_indexes_j[~same_pt_mask]\n",
    "vis_scores_i = vis_scores_i[~same_pt_mask]\n",
    "vis_scores_j = vis_scores_j[~same_pt_mask]\n",
    "conf_scores_i = conf_scores_i[~same_pt_mask]\n",
    "conf_scores_j = conf_scores_j[~same_pt_mask]\n",
    "\n",
    "# depths_i = depth_map[frame_indexes_i, np.around(corr_points_i[:, 1]).astype(int), np.around(corr_points_i[:, 0]).astype(int)]\n",
    "# depths_j = depth_map[frame_indexes_j, np.around(corr_points_j[:, 1]).astype(int), np.around(corr_points_j[:, 0]).astype(int)]\n",
    "# corr_points_i = np.concatenate([corr_points_i, depths_i], axis=1)\n",
    "# corr_points_j = np.concatenate([corr_points_j, depths_j], axis=1)\n",
    "\n",
    "intrinsic_i = np.zeros((corr_points_i.shape[0], 4, 4), dtype=np.float32)\n",
    "intrinsic_j = np.zeros((corr_points_j.shape[0], 4, 4), dtype=np.float32)\n",
    "intrinsic_i[:, :3, :3] = intrinsic[frame_indexes_i]\n",
    "intrinsic_j[:, :3, :3] = intrinsic[frame_indexes_j]\n",
    "intrinsic_i[:, 3, 3] = 1.0\n",
    "intrinsic_j[:, 3, 3] = 1.0\n",
    "\n",
    "extrinsic_i = np.zeros((corr_points_i.shape[0], 4, 4), dtype=np.float32)\n",
    "extrinsic_j = np.zeros((corr_points_j.shape[0], 4, 4), dtype=np.float32)\n",
    "extrinsic_i[:, :3, :4] = extrinsic[frame_indexes_i]\n",
    "extrinsic_j[:, :3, :4] = extrinsic[frame_indexes_j]\n",
    "extrinsic_i[:, 3, 3] = 1.0\n",
    "extrinsic_j[:, 3, 3] = 1.0\n",
    "\n",
    "corr_points_i_tensor = torch.FloatTensor(corr_points_i).to(device)\n",
    "corr_points_j_tensor = torch.FloatTensor(corr_points_j).to(device)\n",
    "weight_i = torch.FloatTensor(vis_scores_i * conf_scores_i).to(device)\n",
    "weight_j = torch.FloatTensor(vis_scores_j * conf_scores_j).to(device)\n",
    "intrinsic_i_tensor = torch.FloatTensor(intrinsic_i).to(device)\n",
    "intrinsic_j_tensor = torch.FloatTensor(intrinsic_j).to(device)\n",
    "extrinsic_i_tensor = torch.FloatTensor(extrinsic_i).to(device)\n",
    "extrinsic_j_tensor = torch.FloatTensor(extrinsic_j).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    P_i = intrinsic_i_tensor @ extrinsic_i_tensor\n",
    "    P_j = intrinsic_j_tensor @ extrinsic_j_tensor\n",
    "    Fm = kornia.geometry.epipolar.fundamental_from_projections(P_i[:, :3], P_j[:, :3])\n",
    "    err = kornia.geometry.symmetrical_epipolar_distance(corr_points_i_tensor[:, None, :2], corr_points_j_tensor[:, None, :2], Fm, squared=False, eps=1e-08)\n",
    "    weight = torch.sqrt(weight_i * weight_j)\n",
    "    err = err * weight[:, None] / weight.mean()\n",
    "\n",
    "# show the error distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(err.cpu().numpy(), bins=100, density=True)\n",
    "plt.title(f\"Symmetrical Epipolar Distance Distribution of {len(err)} correspondences, average: {err.mean().item():.3f}, std: {err.std().item():.3f}\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sampled_i = torch.zeros((corr_points_i_tensor.shape[0], 3), dtype=images.dtype, device=images.device)\n",
    "sampled_j = torch.zeros((corr_points_j_tensor.shape[0], 3), dtype=images.dtype, device=images.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    corr_points_i_normalized = corr_points_i_tensor.to(images.device) / original_coords[frame_indexes_i][:, 2:4] * 2 - 1\n",
    "    corr_points_j_normalized = corr_points_j_tensor.to(images.device) / original_coords[frame_indexes_j][:, 2:4] * 2 - 1\n",
    "\n",
    "    for frame_idx in tqdm(np.unique(frame_indexes_i)):\n",
    "        sampled_i[frame_indexes_i==frame_idx] = F.grid_sample(\n",
    "            images[frame_idx].unsqueeze(0),\n",
    "            corr_points_i_normalized[frame_indexes_i==frame_idx][None, None],\n",
    "            align_corners=True,\n",
    "            mode='bilinear'\n",
    "        ).squeeze().permute(1, 0)\n",
    "\n",
    "    for frame_idx in tqdm(np.unique(frame_indexes_j)):\n",
    "        sampled_j[frame_indexes_j==frame_idx] = F.grid_sample(\n",
    "            images[frame_idx].unsqueeze(0),\n",
    "            corr_points_j_normalized[frame_indexes_j==frame_idx][None, None],\n",
    "            align_corners=True,\n",
    "            mode='bilinear'\n",
    "        ).squeeze().permute(1, 0)\n",
    "\n",
    "    err_rgb = torch.norm(sampled_i - sampled_j, dim=-1)\n",
    "# show the error distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(err_rgb.cpu().numpy(), bins=100, density=True)\n",
    "plt.title(f\"RGB Distance Distribution of {len(err_rgb)} correspondences, average: {err_rgb.mean().item():.3f}, std: {err_rgb.std().item():.3f}\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align with 3DGS training\n",
    "intrinsic_i_normalized = intrinsic_i.copy()\n",
    "intrinsic_j_normalized = intrinsic_j.copy()\n",
    "intrinsic_i_normalized[:, :2] /= original_coords[frame_indexes_i][:, 2:4, None]\n",
    "intrinsic_j_normalized[:, :2] /= original_coords[frame_indexes_j][:, 2:4, None]\n",
    "\n",
    "image_path_list_track = [os.path.join(images_dir+\"_4\", images_gt_updated[id].name) for id in images_gt_updated.keys()]\n",
    "images, original_coords_track = load_and_preprocess_images_ratio(image_path_list_track)\n",
    "\n",
    "intrinsic_i_tensor_ = torch.FloatTensor(intrinsic_i_normalized)\n",
    "intrinsic_j_tensor_ = torch.FloatTensor(intrinsic_j_normalized)\n",
    "intrinsic_i_tensor_[:, :2] *= original_coords_track[frame_indexes_i][:, 2:4, None]\n",
    "intrinsic_j_tensor_[:, :2] *= original_coords_track[frame_indexes_j][:, 2:4, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align with 3DGS training\n",
    "intrinsic_i_normalized = intrinsic_i.copy()\n",
    "intrinsic_j_normalized = intrinsic_j.copy()\n",
    "intrinsic_i_normalized[:, :2] /= original_coords[frame_indexes_i][:, 2:4, None]\n",
    "intrinsic_j_normalized[:, :2] /= original_coords[frame_indexes_j][:, 2:4, None]\n",
    "\n",
    "intrinsic_i_tensor_ = torch.FloatTensor(intrinsic_i_normalized)\n",
    "intrinsic_j_tensor_ = torch.FloatTensor(intrinsic_j_normalized)\n",
    "intrinsic_i_tensor_[:, :2] *= original_coords_track[frame_indexes_i][:, 2:4, None]\n",
    "intrinsic_j_tensor_[:, :2] *= original_coords_track[frame_indexes_j][:, 2:4, None]\n",
    "intrinsic_i_tensor_ = intrinsic_i_tensor_.to(intrinsic_i_tensor.device)\n",
    "intrinsic_j_tensor_ = intrinsic_j_tensor_.to(intrinsic_j_tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 53.93it/s]\n",
      "100%|██████████| 131/131 [00:01<00:00, 129.06it/s]\n",
      "/tmp/ipykernel_3872/1444616246.py:59: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3872/1444616246.py:70: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "search_radius = 3  # search radius in pixels\n",
    "S, C, H, W = images.shape\n",
    "sampled_i = torch.zeros((corr_points_i_normalized.shape[0], 3), dtype=images.dtype, device=images.device)\n",
    "sampled_j = torch.zeros((corr_points_j_normalized.shape[0], 3), dtype=images.dtype, device=images.device)\n",
    "\n",
    "corr_points_i_normalized_ = torch.zeros_like(corr_points_i_normalized, device=images.device)\n",
    "\n",
    "for frame_idx in tqdm(np.unique(frame_indexes_j)):\n",
    "    sampled_j[frame_indexes_j==frame_idx] = F.grid_sample(\n",
    "        images[frame_idx].unsqueeze(0),\n",
    "        corr_points_j_normalized[frame_indexes_j==frame_idx][None, None],\n",
    "        align_corners=True,\n",
    "        mode='bilinear'\n",
    "    ).squeeze().permute(1, 0)\n",
    "\n",
    "for frame_idx in tqdm(np.unique(frame_indexes_i)):\n",
    "    offsets = torch.stack(torch.meshgrid(\n",
    "        torch.arange(-search_radius * 2 / W, (search_radius + 1) * 2 / W, 2 / W, device=images.device),\n",
    "        torch.arange(-search_radius * 2 / H, (search_radius + 1) * 2 / H, 2 / H, device=images.device),\n",
    "        indexing='xy'\n",
    "    ), dim=-1).view(-1, 2)\n",
    "    \n",
    "    corr_points_i_normalized_offset = corr_points_i_normalized[frame_indexes_i==frame_idx][:, None, :] + offsets[None, :, :]\n",
    "\n",
    "    sampled_i_temp = F.grid_sample(\n",
    "        images[frame_idx].unsqueeze(0),\n",
    "        corr_points_i_normalized_offset[None],\n",
    "        align_corners=True,\n",
    "        mode='bilinear'\n",
    "    ).squeeze(0).permute(1, 2, 0)\n",
    "\n",
    "    rgb_diff = torch.norm(sampled_i_temp - sampled_j[frame_indexes_i==frame_idx][:, None, :], dim=-1)\n",
    "    min_diff_idx = rgb_diff.argmin(dim=1)\n",
    "\n",
    "    sampled_i[frame_indexes_i==frame_idx] = sampled_i_temp[torch.arange(sampled_i_temp.shape[0]), min_diff_idx]\n",
    "    corr_points_i_normalized_[frame_indexes_i==frame_idx] = corr_points_i_normalized_offset[torch.arange(corr_points_i_normalized_offset.shape[0]), min_diff_idx]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    P_i = intrinsic_i_tensor_ @ extrinsic_i_tensor\n",
    "    P_j = intrinsic_j_tensor_ @ extrinsic_j_tensor\n",
    "    \n",
    "    corr_points_i_tensor_ = (corr_points_i_normalized_ + 1) * original_coords_track[frame_indexes_i][:, 2:4] / 2\n",
    "    corr_points_j_tensor_ = (corr_points_j_normalized + 1) * original_coords_track[frame_indexes_j][:, 2:4] / 2\n",
    "    corr_points_i_tensor_ = corr_points_i_tensor_.to(P_i.device)\n",
    "    corr_points_j_tensor_ = corr_points_j_tensor_.to(P_j.device)\n",
    "\n",
    "    Fm = kornia.geometry.epipolar.fundamental_from_projections(P_i[:, :3], P_j[:, :3])\n",
    "    err = kornia.geometry.symmetrical_epipolar_distance(corr_points_i_tensor_[:, None, :2], corr_points_j_tensor_[:, None, :2], Fm, squared=False, eps=1e-08)\n",
    "    weight = torch.sqrt(weight_i * weight_j)\n",
    "    err = err * weight[:, None] / weight.mean()\n",
    "\n",
    "# show the error distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(err.cpu().numpy(), bins=100, density=True)\n",
    "plt.title(f\"Symmetrical Epipolar Distance Distribution of {len(err)} correspondences, average: {err.mean().item():.3f}, std: {err.std().item():.3f}, search_radius: {search_radius}\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "err_rgb = torch.norm(sampled_i - sampled_j, dim=-1)\n",
    "\n",
    "# show the error distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(err_rgb.cpu().numpy(), bins=100, density=True)\n",
    "plt.title(f\"RGB Distance Distribution of {len(err_rgb)} correspondences, average: {err_rgb.mean().item():.3f},  std: {err_rgb.std().item():.3f}, search_radius: {search_radius}\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point correspondences saved to ../data/MipNeRF360_vggt/treehill/corr_s3.npy.\n"
     ]
    }
   ],
   "source": [
    "save_dir = sparse_dir_pred.split('/sparse/')[0]\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "np.save(os.path.join(save_dir, f'corr_s{search_radius}.npy'), {\n",
    "    'corr_points_i_normalized': corr_points_i_normalized_.cpu().numpy(),\n",
    "    'corr_points_j_normalized': corr_points_j_normalized.cpu().numpy(),\n",
    "    'image_names_i': image_names_i,\n",
    "    'image_names_j': image_names_j,\n",
    "})\n",
    "\n",
    "print(f\"Point correspondences saved to {os.path.join(save_dir, f'corr_s{search_radius}.npy')}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vggt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
