{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import roma\n",
    "import kornia\n",
    "import utils.colmap as colmap_utils\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images_ratio, load_and_preprocess_images_square\n",
    "from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "\n",
    "from utils.umeyama import umeyama\n",
    "from utils.metric_torch import camera_to_rel_deg, calculate_auc_np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_VGGT(model, images, dtype, resolution=518, track_feat=False):\n",
    "    # images: [B, 3, H, W]\n",
    "\n",
    "    assert len(images.shape) == 4\n",
    "    assert images.shape[1] == 3\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    images = images.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(dtype=dtype):\n",
    "            images = images[None]  # add batch dimension\n",
    "            valid_layers = model.depth_head.intermediate_layer_idx\n",
    "            if valid_layers[-1] != model.aggregator.aa_block_num - 1:\n",
    "                valid_layers.append(model.aggregator.aa_block_num - 1)\n",
    "            aggregated_tokens_list, ps_idx = model.aggregator(images, valid_layers)\n",
    "            aggregated_tokens_list = [tokens.to(device) if tokens is not None else None for tokens in aggregated_tokens_list]\n",
    "\n",
    "        # Predict Cameras\n",
    "        pose_enc = model.camera_head(aggregated_tokens_list)[-1]\n",
    "        # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)\n",
    "        extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])\n",
    "        # Predict Depth Maps\n",
    "        depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)\n",
    "\n",
    "        extrinsic = extrinsic.squeeze(0).cpu().numpy()\n",
    "        intrinsic = intrinsic.squeeze(0).cpu().numpy()\n",
    "        depth_map = depth_map.squeeze(0).cpu().numpy()\n",
    "        depth_conf = depth_conf.squeeze(0).cpu().numpy()\n",
    "\n",
    "        track_feature_maps = None if not track_feat else model.track_head.feature_extractor(aggregated_tokens_list, images, ps_idx)\n",
    "        \n",
    "    return extrinsic, intrinsic, depth_map, depth_conf, track_feature_maps\n",
    "\n",
    "def evaluate_auc(pred_se3, gt_se3, device):\n",
    "\n",
    "    camera_centers_gt = - (gt_se3[:, :3, :3].cpu().numpy().transpose(0, 2, 1) @ gt_se3[:, 3, :3][..., None].cpu().numpy()).squeeze(-1)\n",
    "    camera_centers_pred = - (pred_se3[:, :3, :3].cpu().numpy().transpose(0, 2, 1) @ pred_se3[:, 3, :3][..., None].cpu().numpy()).squeeze(-1)\n",
    "    c, R, t = umeyama(camera_centers_gt.T, camera_centers_pred.T)\n",
    "    camera_centers_gt_aligned = (c * (R @ camera_centers_gt.T) + t).T\n",
    "    print(\"    --  Umeyama Scale: \", c)\n",
    "    print(\"    --  Umeyama Rotation: \\n\", R)\n",
    "    print(\"    --  Umeyama Translation: \\n\", t)\n",
    "\n",
    "    ext_transform = np.eye(4)\n",
    "    ext_transform[:3, :3] = R\n",
    "    ext_transform[:3, 3:] = t\n",
    "    ext_transform = np.linalg.inv(ext_transform)\n",
    "\n",
    "    gt_aligned = np.zeros((gt_se3.shape[0], 4, 4))\n",
    "    gt_aligned[:, :3, :3] = gt_se3[:, :3, :3].cpu().numpy()\n",
    "    gt_aligned[:, :3, 3] = gt_se3[:, 3, :3].cpu().numpy() * c\n",
    "    gt_aligned[:, 3, 3] = 1.0\n",
    "    gt_aligned = np.einsum('bmn,bnk->bmk', gt_aligned, ext_transform[None])\n",
    "\n",
    "    gt_se3_aligned = torch.eye(4, device=device).unsqueeze(0).repeat(len(gt_se3), 1, 1)\n",
    "    gt_se3_aligned[:, :3, :3] = torch.tensor(gt_aligned[:, :3, :3], device=device)\n",
    "    gt_se3_aligned[:, 3, :3] = torch.tensor(gt_aligned[:, :3, 3], device=device)\n",
    "\n",
    "    rel_rangle_deg, rel_tangle_deg = camera_to_rel_deg(pred_se3, gt_se3_aligned, device, 4)\n",
    "    print(f\"    --  Pair Rot   Error (Deg) of Vanilla: {rel_rangle_deg.mean():10.2f}\")\n",
    "    print(f\"    --  Pair Trans Error (Deg) of Vanilla: {rel_tangle_deg.mean():10.2f}\")\n",
    "\n",
    "    rError = rel_rangle_deg.cpu().numpy()\n",
    "    tError = rel_tangle_deg.cpu().numpy()\n",
    "\n",
    "    Auc_30 = calculate_auc_np(rError, tError, max_threshold=30)\n",
    "    print(f\"    --  AUC at 30: {Auc_30:.4f}\")\n",
    "\n",
    "    results = {\n",
    "        'rel_rangle_deg': rel_rangle_deg.mean(),\n",
    "        'rel_tangle_deg': rel_tangle_deg.mean(),\n",
    "        'Auc_30': Auc_30,\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "def image_pair_candidates(extrinsic, pairing_angle_threshold=30, unique_pairs=False):\n",
    "\n",
    "    pairs, pairs_cnt = {}, 0\n",
    "\n",
    "    # assert i_map is None or len(i_map) == len(extrinsics)\n",
    "\n",
    "    num_images = len(extrinsic)\n",
    "    \n",
    "    extrinsic_tensor = torch.from_numpy(extrinsic)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        \n",
    "        rot_mat_i = extrinsic_tensor[i:i+1, :3, :3]\n",
    "        rot_mats_j = extrinsic_tensor[i+1:, :3, :3]\n",
    "\n",
    "        rot_mat_ij = torch.matmul(rot_mat_i, torch.linalg.inv(rot_mats_j))\n",
    "        angle_rad = torch.acos((rot_mat_ij.diagonal(offset=0, dim1=-1, dim2=-2).sum(-1) - 1) / 2)\n",
    "        angle_deg = angle_rad / np.pi * 180\n",
    "\n",
    "        i_entry = i\n",
    "        j_entries = (i + 1 + torch.where(torch.abs(angle_deg) < pairing_angle_threshold)[0]).tolist()\n",
    "\n",
    "        # sort entries by the value of the angle\n",
    "        j_entries = sorted(j_entries, key=lambda x: angle_deg[x - i - 1].item())\n",
    "\n",
    "        pairs_cnt += len(j_entries)\n",
    "\n",
    "        if not i_entry in pairs.keys():\n",
    "            pairs[i_entry] = []\n",
    "        pairs[i_entry] = pairs[i_entry] + j_entries\n",
    "\n",
    "        if not unique_pairs:\n",
    "            for j_entry in j_entries:\n",
    "                if not j_entry in pairs.keys():\n",
    "                    pairs[j_entry] = []\n",
    "                pairs[j_entry].append(i_entry)\n",
    "\n",
    "    return pairs, pairs_cnt\n",
    "\n",
    "def warp_corners_and_draw_matches(ref_points, dst_points, img1, img2):\n",
    "    # Calculate the Homography matrix\n",
    "    H, mask = cv2.findHomography(ref_points, dst_points, cv2.USAC_MAGSAC, 3.5, maxIters=1_000, confidence=0.999)\n",
    "    mask = mask.flatten()\n",
    "\n",
    "    if torch.is_tensor(img1):\n",
    "        img1 = img1[0].permute(1, 2, 0).cpu().numpy()\n",
    "        if img1.max() > 1.0:\n",
    "            img1 = img1.astype(np.uint8)\n",
    "        else:\n",
    "            img1 = (img1 * 255).astype(np.uint8)\n",
    "    if torch.is_tensor(img2):\n",
    "        img2 = img2[0].permute(1, 2, 0).cpu().numpy()\n",
    "        if img2.max() > 1.0:\n",
    "            img2 = img2.astype(np.uint8)\n",
    "        else:\n",
    "            img2 = (img2 * 255).astype(np.uint8)\n",
    "\n",
    "    # Get corners of the first image (image1)\n",
    "    h, w = img1.shape[:2]\n",
    "    corners_img1 = np.array([[0, 0], [w-1, 0], [w-1, h-1], [0, h-1]], dtype=np.float32).reshape(-1, 1, 2)\n",
    "\n",
    "    # Warp corners to the second image (image2) space\n",
    "    warped_corners = cv2.perspectiveTransform(corners_img1, H)\n",
    "\n",
    "    # Draw the warped corners in image2\n",
    "    img2_with_corners = img2.copy()\n",
    "    for i in range(len(warped_corners)):\n",
    "        start_point = tuple(warped_corners[i-1][0].astype(int))\n",
    "        end_point = tuple(warped_corners[i][0].astype(int))\n",
    "        cv2.line(img2_with_corners, start_point, end_point, (0, 255, 0), 4)  # Using solid green for corners\n",
    "\n",
    "    # Prepare keypoints and matches for drawMatches function\n",
    "    keypoints1 = [cv2.KeyPoint(p[0], p[1], 5) for p in ref_points]\n",
    "    keypoints2 = [cv2.KeyPoint(p[0], p[1], 5) for p in dst_points]\n",
    "    matches = [cv2.DMatch(i,i,0) for i in range(len(mask)) if mask[i]]\n",
    "\n",
    "    # Draw inlier matches\n",
    "    img_matches = cv2.drawMatches(img1, keypoints1, img2_with_corners, keypoints2, matches, None,\n",
    "                                  matchColor=(0, 255, 0), flags=2)\n",
    "\n",
    "    return img_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGGT(\n",
       "  (aggregator): Aggregator(\n",
       "    (patch_embed): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x NestedTensorBlock(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MemEffAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (rope): RotaryPositionEmbedding2D()\n",
       "    (frame_blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): RotaryPositionEmbedding2D()\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (global_blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (rope): RotaryPositionEmbedding2D()\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (camera_head): CameraHead(\n",
       "    (trunk): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (token_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (trunk_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (embed_pose): Linear(in_features=9, out_features=2048, bias=True)\n",
       "    (poseLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "    )\n",
       "    (adaln_norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=False)\n",
       "    (pose_branch): Mlp(\n",
       "      (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (fc2): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (drop): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (point_head): DPTHead(\n",
       "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (depth_head): DPTHead(\n",
       "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (projects): ModuleList(\n",
       "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (resize_layers): ModuleList(\n",
       "      (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): Identity()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (scratch): Module(\n",
       "      (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (refinenet1): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet2): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet3): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit1): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (refinenet4): FeatureFusionBlock(\n",
       "        (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (resConfUnit2): ResidualConvUnit(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation): ReLU(inplace=True)\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (output_conv2): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (track_head): TrackHead(\n",
       "    (feature_extractor): DPTHead(\n",
       "      (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (projects): ModuleList(\n",
       "        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2-3): 2 x Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (resize_layers): ModuleList(\n",
       "        (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (2): Identity()\n",
       "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (scratch): Module(\n",
       "        (layer1_rn): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer2_rn): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer3_rn): Conv2d(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer4_rn): Conv2d(1024, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (refinenet1): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet2): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet3): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit1): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (refinenet4): FeatureFusionBlock(\n",
       "          (out_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resConfUnit2): ResidualConvUnit(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (activation): ReLU(inplace=True)\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (skip_add): FloatFunctional(\n",
       "            (activation_post_process): Identity()\n",
       "          )\n",
       "        )\n",
       "        (output_conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (tracker): BaseTrackerPredictor(\n",
       "      (corr_mlp): Mlp(\n",
       "        (fc1): Linear(in_features=567, out_features=384, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (updateformer): EfficientUpdateFormer(\n",
       "        (input_norm): LayerNorm((388,), eps=1e-05, elementwise_affine=True)\n",
       "        (input_transform): Linear(in_features=388, out_features=384, bias=True)\n",
       "        (output_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (flow_head): Linear(in_features=384, out_features=130, bias=True)\n",
       "        (time_blocks): ModuleList(\n",
       "          (0-5): 6 x AttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_virtual_blocks): ModuleList(\n",
       "          (0-5): 6 x AttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_point2virtual_blocks): ModuleList(\n",
       "          (0-5): 6 x CrossAttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_context): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (space_virtual2point_blocks): ModuleList(\n",
       "          (0-5): 6 x CrossAttnBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_context): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fmap_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffeat_norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "      (ffeat_updater): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "      )\n",
       "      (vis_predictor): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "      (conf_predictor): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model and load the pretrained weights.\n",
    "# This will automatically download the model weights the first time it's run, which may take a while.\n",
    "model = VGGT.from_pretrained(\"facebook/VGGT-1B\").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image paths and preprocess them\n",
    "data_dir = \"../data/MipNeRF360/bonsai\"\n",
    "sparse_dir_gt = os.path.join(data_dir, \"sparse\", \"0\")\n",
    "dust_dir = os.path.join(data_dir, \"mast3r\")\n",
    "images_dir = os.path.join(data_dir, \"images\")\n",
    "\n",
    "cameras_gt = colmap_utils.read_cameras_binary(os.path.join(sparse_dir_gt, \"cameras.bin\"))\n",
    "images_gt = colmap_utils.read_images_binary(os.path.join(sparse_dir_gt, \"images.bin\"))\n",
    "pcd_gt = colmap_utils.read_points3D_binary(os.path.join(sparse_dir_gt, \"points3D.bin\"))\n",
    "# images_gt = dict(sorted(images_gt.items(), key=lambda item: item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_interval = 1\n",
    "images_gt_updated = {id: images_gt[id] for id in list(images_gt.keys())[::sample_interval]}\n",
    "image_path_list = [os.path.join(images_dir, images_gt_updated[id].name) for id in images_gt_updated.keys()]\n",
    "base_image_path_list = [os.path.basename(path) for path in image_path_list]\n",
    "total_frame_num = len(image_path_list)\n",
    "\n",
    "vggt_fixed_resolution = 518\n",
    "images, original_coords = load_and_preprocess_images_ratio(image_path_list, vggt_fixed_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.31 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run VGGT to estimate camera and depth\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Run with 518x518 images\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m extrinsic, intrinsic, depth_map, depth_conf, track_feats \u001b[38;5;241m=\u001b[39m \u001b[43mrun_VGGT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvggt_fixed_resolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m points_3d \u001b[38;5;241m=\u001b[39m unproject_depth_map_to_point_map(depth_map, extrinsic, intrinsic)\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[21], line 31\u001b[0m, in \u001b[0;36mrun_VGGT\u001b[0;34m(model, images, dtype, resolution, track_feat)\u001b[0m\n\u001b[1;32m     28\u001b[0m     depth_map \u001b[38;5;241m=\u001b[39m depth_map\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     29\u001b[0m     depth_conf \u001b[38;5;241m=\u001b[39m depth_conf\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 31\u001b[0m     track_feature_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m track_feat \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maggregated_tokens_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mps_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extrinsic, intrinsic, depth_map, depth_conf, track_feature_maps\n",
      "File \u001b[0;32m/data1/jing_li/anaconda3/envs/vggt_310_bkup/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data1/jing_li/anaconda3/envs/vggt_310_bkup/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data1/yang_liu/python_workspace/vggt/notebooks/../vggt/heads/dpt_head.py:168\u001b[0m, in \u001b[0;36mDPTHead.forward\u001b[0;34m(self, aggregated_tokens_list, images, patch_start_idx, frames_chunk_size)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Concatenate results along the sequence dimension\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_only:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(all_preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mcat(all_conf, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.31 GiB. GPU "
     ]
    }
   ],
   "source": [
    "# Run VGGT to estimate camera and depth\n",
    "# Run with 518x518 images\n",
    "extrinsic, intrinsic, depth_map, depth_conf, track_feats = run_VGGT(model, images, dtype, vggt_fixed_resolution, track_feat=True)\n",
    "points_3d = unproject_depth_map_to_point_map(depth_map, extrinsic, intrinsic)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9056695  -0.00345442 -0.42397028  0.23630652]\n",
      " [-0.16402441  0.9192514  -0.35787264  0.24832754]\n",
      " [ 0.39097154  0.39365584  0.83197135  0.07307453]]\n"
     ]
    }
   ],
   "source": [
    "print(extrinsic[255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    --  Umeyama Scale:  0.1286321755249738\n",
      "    --  Umeyama Rotation: \n",
      " [[ 0.45210756  0.64201917 -0.61920117]\n",
      " [-0.2451966   0.75691172  0.605775  ]\n",
      " [ 0.85759981 -0.12204945  0.49962644]]\n",
      "    --  Umeyama Translation: \n",
      " [[ 0.0348455 ]\n",
      " [-0.29861492]\n",
      " [ 0.46608338]]\n",
      "    --  Pair Rot   Error (Deg) of Vanilla:       1.58\n",
      "    --  Pair Trans Error (Deg) of Vanilla:       2.40\n",
      "    --  AUC at 30: 0.9239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1205/4258586071.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  fl_gt = torch.tensor([cameras_gt[image.camera_id].params[0:2] for image in images_gt_updated.values()], device=device)\n"
     ]
    }
   ],
   "source": [
    "fl_gt = torch.tensor([cameras_gt[image.camera_id].params[0:2] for image in images_gt_updated.values()], device=device)\n",
    "translation_gt = torch.tensor([image.tvec for image in images_gt_updated.values()], device=device)\n",
    "rotation_gt = torch.tensor([colmap_utils.qvec2rotmat(image.qvec) for image in images_gt_updated.values()], device=device)\n",
    "\n",
    "# gt w2c\n",
    "gt_se3 = torch.eye(4, device=device).unsqueeze(0).repeat(len(images_gt_updated), 1, 1)\n",
    "gt_se3[:, :3, :3] = rotation_gt\n",
    "gt_se3[:, 3, :3] = translation_gt\n",
    "\n",
    "# pred w2c\n",
    "pred_se3 = torch.eye(4, device=device).unsqueeze(0).repeat(len(images_gt_updated), 1, 1)\n",
    "pred_se3[:, :3, :3] = torch.tensor(extrinsic[:, :3, :3], device=device)\n",
    "pred_se3[:, 3, :3] = torch.tensor(extrinsic[:, :3, 3], device=device)\n",
    "\n",
    "results = evaluate_auc(pred_se3, gt_se3, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculate Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def extract_matches(extrinsic, intrinsic, images, base_image_path_list, max_query_pts=4096):\n",
    "\n",
    "    xfeat = torch.hub.load('/home/jing_li/.cache/torch/hub/verlab_accelerated_features_main', \n",
    "                           'XFeat', source='local', pretrained=True, top_k=max_query_pts)\n",
    "\n",
    "    pairs, pairs_cnt = image_pair_candidates(extrinsic, pairing_angle_threshold=30, unique_pairs=True)\n",
    "    print(\"Total candidate image pairs found: \", pairs_cnt)\n",
    "\n",
    "    indexes_i = list(range(len(base_image_path_list)-1))  # the last image \n",
    "    # indexes_j = [np.random.choice(pairs[idx_i], min(20, len(pairs[idx_i])), replace=False) for idx_i in indexes_i]\n",
    "    indexes_j = [pairs[idx_i] for idx_i in indexes_i]\n",
    "    indexes_i = [np.array([idx_i] * len(indexes_j[idx_i])) for idx_i in indexes_i]\n",
    "    indexes_i = np.concatenate(indexes_i).tolist()\n",
    "    indexes_j = np.concatenate(indexes_j).tolist()\n",
    "\n",
    "    batch_size, matches_list = 100, []\n",
    "\n",
    "    for i in tqdm(range(0, len(indexes_i), batch_size), desc=\"Matching image pairs...\"):\n",
    "        indexes_i_batch = indexes_i[i:i + batch_size]\n",
    "        indexes_j_batch = indexes_j[i:i + batch_size]\n",
    "        \n",
    "        # Extract features for the batch\n",
    "        images_i = images[indexes_i_batch]\n",
    "        images_j = images[indexes_j_batch]\n",
    "        \n",
    "        # Match features\n",
    "        matches_batch = xfeat.match_xfeat_star(images_i, images_j)\n",
    "        matches_list.extend(matches_batch)\n",
    "\n",
    "    num_matches = [len(m) for m in matches_list]\n",
    "\n",
    "    indexes_i_expanded = []\n",
    "    indexes_j_expanded = []\n",
    "\n",
    "    for idx, n in enumerate(num_matches):\n",
    "        indexes_i_expanded.append(np.array([indexes_i[idx]] * n, dtype=np.int64))\n",
    "        indexes_j_expanded.append(np.array([indexes_j[idx]] * n, dtype=np.int64))\n",
    "    indexes_i_expanded = np.concatenate(indexes_i_expanded)\n",
    "    indexes_j_expanded = np.concatenate(indexes_j_expanded)\n",
    "\n",
    "    image_names_i = np.array(base_image_path_list)[indexes_i_expanded]\n",
    "    image_names_j = np.array(base_image_path_list)[indexes_j_expanded]\n",
    "\n",
    "    corr_points_i = torch.cat([matches_list[k][:, :2] for k in range(len(matches_list))], dim=0).cpu()\n",
    "    corr_points_j = torch.cat([matches_list[k][:, 2:] for k in range(len(matches_list))], dim=0).cpu()\n",
    "\n",
    "    intrinsic_i = np.zeros((corr_points_i.shape[0], 4, 4), dtype=np.float32)\n",
    "    intrinsic_j = np.zeros((corr_points_j.shape[0], 4, 4), dtype=np.float32)\n",
    "    intrinsic_i[:, :3, :3] = intrinsic[indexes_i_expanded]\n",
    "    intrinsic_j[:, :3, :3] = intrinsic[indexes_j_expanded]\n",
    "    intrinsic_i[:, 3, 3] = 1.0\n",
    "    intrinsic_j[:, 3, 3] = 1.0\n",
    "\n",
    "    extrinsic_i = np.zeros((corr_points_i.shape[0], 4, 4), dtype=np.float32)\n",
    "    extrinsic_j = np.zeros((corr_points_j.shape[0], 4, 4), dtype=np.float32)\n",
    "    extrinsic_i[:, :3, :4] = extrinsic[indexes_i_expanded]\n",
    "    extrinsic_j[:, :3, :4] = extrinsic[indexes_j_expanded]\n",
    "    extrinsic_i[:, 3, 3] = 1.0\n",
    "    extrinsic_j[:, 3, 3] = 1.0\n",
    "\n",
    "    device = corr_points_i.device\n",
    "\n",
    "    intrinsic_i_tensor = torch.FloatTensor(intrinsic_i).to(device)\n",
    "    intrinsic_j_tensor = torch.FloatTensor(intrinsic_j).to(device)\n",
    "    extrinsic_i_tensor = torch.FloatTensor(extrinsic_i).to(device)\n",
    "    extrinsic_j_tensor = torch.FloatTensor(extrinsic_j).to(device)\n",
    "\n",
    "    P_i = intrinsic_i_tensor @ extrinsic_i_tensor\n",
    "    P_j = intrinsic_j_tensor @ extrinsic_j_tensor\n",
    "    Fm = kornia.geometry.epipolar.fundamental_from_projections(P_i[:, :3], P_j[:, :3])\n",
    "    err = kornia.geometry.symmetrical_epipolar_distance(corr_points_i[:, None, :2], corr_points_j[:, None, :2], Fm, squared=False, eps=1e-08)\n",
    "    \n",
    "    hist, bin_edges = torch.histogram(err.cpu(), bins=100, range=(0, 20), density=True)  # move to cpu to avoid CUDA \"backend\"\n",
    "    corr_weights = torch.zeros_like(err)\n",
    "    for i in range(len(bin_edges) - 1):\n",
    "        mask = (err >= bin_edges[i]) & (err < bin_edges[i + 1])\n",
    "        if torch.any(mask):\n",
    "            corr_weights[mask] = (hist[i] * (bin_edges[i + 1] - bin_edges[i])) / (bin_edges[-1] - bin_edges[0])\n",
    "    corr_weights /= corr_weights.mean()\n",
    "    \n",
    "    # set corr_weights to 0 for points outside the image frame\n",
    "    in_frame_i = (corr_points_i[..., 0] > images.shape[-1]) & (corr_points_i[..., 0] < 0) & \\\n",
    "                    (corr_points_i[..., 1] > images.shape[-2]) & (corr_points_i[..., 1] < 0)\n",
    "    in_frame_j = (corr_points_j[..., 0] > images.shape[-1]) & (corr_points_j[..., 0] < 0) & \\\n",
    "                    (corr_points_j[..., 1] > images.shape[-2]) & (corr_points_j[..., 1] < 0)\n",
    "    corr_weights[in_frame_i & in_frame_j] = 0.0\n",
    "    \n",
    "    # rearrange corr_points_i_normalized and corr_points_j_normalized to (P, N, 2)\n",
    "    P, N = len(num_matches), max(num_matches)\n",
    "    corr_points_i_batched = torch.zeros((P, N, 2), dtype=corr_points_i.dtype, device=corr_points_i.device)\n",
    "    corr_points_j_batched = torch.zeros((P, N, 2), dtype=corr_points_j.dtype, device=corr_points_j.device)\n",
    "    corr_weights_batched = torch.zeros((P, N, 1), dtype=corr_weights.dtype, device=corr_weights.device)\n",
    "    image_names_i_batched = np.zeros((P), dtype=image_names_i.dtype)\n",
    "    image_names_j_batched = np.zeros((P), dtype=image_names_j.dtype)\n",
    "\n",
    "    start_idx = 0\n",
    "    for p in range(P):\n",
    "        end_idx = start_idx + num_matches[p]\n",
    "        corr_points_i_batched[p, :num_matches[p]] = corr_points_i[start_idx:end_idx]\n",
    "        corr_points_j_batched[p, :num_matches[p]] = corr_points_j[start_idx:end_idx]\n",
    "        corr_weights_batched[p, :num_matches[p]] = corr_weights[start_idx:end_idx]\n",
    "        image_names_i_batched[p] = image_names_i[start_idx]\n",
    "        image_names_j_batched[p] = image_names_j[start_idx]\n",
    "        assert (image_names_i[start_idx:end_idx] == image_names_i_batched[p]).all()\n",
    "        assert (image_names_j[start_idx:end_idx] == image_names_j_batched[p]).all()\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    output_dict = {\n",
    "        \"corr_points_i\": corr_points_i_batched,\n",
    "        \"corr_points_j\": corr_points_j_batched,\n",
    "        \"corr_weights\": corr_weights_batched,\n",
    "        \"image_names_i\": image_names_i_batched,\n",
    "        \"image_names_j\": image_names_j_batched,\n",
    "        \"num_matches\": num_matches,\n",
    "    }\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total candidate image pairs found:  5713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matching image pairs...: 100%|██████████| 49/49 [00:29<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "output = extract_matches(extrinsic, intrinsic, images, base_image_path_list, max_query_pts=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimize Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_K_cam_depth(log_focals, pps, trans, quats, min_focals, max_focals, imsizes):\n",
    "    # make intrinsics\n",
    "    focals = log_focals.exp().clip(min=min_focals, max=max_focals)\n",
    "    K = torch.eye(4, dtype=focals.dtype, device=focals.device)[None].expand(len(trans), 4, 4).clone()\n",
    "    K[:, 0, 0] = K[:, 1, 1] = focals\n",
    "    K[:, 0:2, 2] = pps * imsizes\n",
    "    if trans is None:\n",
    "        return K\n",
    "\n",
    "    w2cs = torch.eye(4, dtype=trans.dtype, device=trans.device)[None].expand(len(trans), 4, 4).clone()\n",
    "    w2cs[:, :3, :3] = roma.unitquat_to_rotmat(F.normalize(quats, dim=1))\n",
    "    w2cs[:, :3, 3] = trans\n",
    "\n",
    "    return K, (w2cs, torch.linalg.inv(w2cs))\n",
    "\n",
    "def cosine_schedule(alpha, lr_base, lr_end=0):\n",
    "    lr = lr_end + (lr_base - lr_end) * (1 + np.cos(alpha * np.pi)) / 2\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate_by_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "def l1_loss(x, y):\n",
    "    return torch.linalg.norm(x - y, dim=-1)\n",
    "\n",
    "def gamma_loss(gamma, mul=1, offset=None, clip=np.inf):\n",
    "    if offset is None:\n",
    "        if gamma == 1:\n",
    "            return l1_loss\n",
    "        # d(x**p)/dx = 1 ==> p * x**(p-1) == 1 ==> x = (1/p)**(1/(p-1))\n",
    "        offset = (1 / gamma)**(1 / (gamma - 1))\n",
    "\n",
    "    def loss_func(x, y):\n",
    "        return (mul * l1_loss(x, y).clip(max=clip) + offset) ** gamma - offset ** gamma\n",
    "    return loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_optimization(match_outputs, \n",
    "                      extrinsic, \n",
    "                      intrinsic, \n",
    "                      images, \n",
    "                      depth_map, \n",
    "                      depth_conf, \n",
    "                      base_image_path_list, \n",
    "                      device='cuda',\n",
    "                      lr_base=5e-4,\n",
    "                      lr_end=1e-5,\n",
    "                      niter=300,\n",
    "                      target_scene_dir=None,\n",
    "                      shared_intrinsics=True):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        imsizes = torch.tensor(images.shape[-2:]).float()\n",
    "        diags = torch.norm(imsizes)\n",
    "        min_focals = 0.25 * diags  # diag = 1.2~1.4*max(W,H) => beta >= 1/(2*1.2*tan(fov/2)) ~= 0.26\n",
    "        max_focals = 10 * diags\n",
    "\n",
    "        qvec = roma.rotmat_to_unitquat(torch.tensor(extrinsic[:, :3, :3]))\n",
    "        tvec = torch.tensor(extrinsic[:, :3, 3])\n",
    "        log_sizes = torch.zeros(len(qvec))\n",
    "\n",
    "        pps = torch.tensor(intrinsic[:, :2, 2]) / imsizes[None, :2]  # default principal_point would be (0.5, 0.5)\n",
    "        base_focals = torch.tensor((intrinsic[:, 0, 0] + intrinsic[:, 1, 1]) / 2)\n",
    "\n",
    "        # intrinsics parameters\n",
    "        if shared_intrinsics:\n",
    "            # Optimize a single set of intrinsics for all cameras. Use averages as init.\n",
    "            confs = depth_conf.mean(axis=(1, 2))\n",
    "            weighting = torch.tensor(confs / confs.sum())\n",
    "            pps = weighting @ pps\n",
    "            pps = pps.view(1, -1).repeat(len(qvec), 1)\n",
    "            focal_m = weighting @ base_focals\n",
    "            log_focals = focal_m.view(1).log().repeat(len(qvec))\n",
    "        else:\n",
    "            log_focals = base_focals.log()\n",
    "        \n",
    "        depth_map_tensor = torch.tensor(depth_map)  # [B, H, W]\n",
    "\n",
    "        corr_points_i = match_outputs[\"corr_points_i\"].clone()\n",
    "        corr_points_j = match_outputs[\"corr_points_j\"].clone()\n",
    "        corr_weights = match_outputs[\"corr_weights\"].clone()\n",
    "        num_matches = match_outputs[\"num_matches\"]\n",
    "\n",
    "        imsizes = imsizes.to(corr_points_i.device)\n",
    "        depth_map_tensor = depth_map_tensor.to(corr_points_i.device)\n",
    "        \n",
    "        corr_points_i_normalized = corr_points_i / imsizes[None, None, [1, 0]] * 2 - 1\n",
    "        corr_points_j_normalized = corr_points_j / imsizes[None, None, [1, 0]] * 2 - 1\n",
    "\n",
    "        indexes_i = [base_image_path_list.index(img_name) for img_name in match_outputs[\"image_names_i\"]]\n",
    "        indexes_j = [base_image_path_list.index(img_name) for img_name in match_outputs[\"image_names_j\"]]\n",
    "\n",
    "        depths_i_list, depths_j_list, depth_batch_size = [], [], 16\n",
    "        for start_idx in range(0, len(corr_points_i_normalized), depth_batch_size):\n",
    "            end_idx = min(start_idx + depth_batch_size, len(corr_points_i_normalized))\n",
    "            depths_i_list.append(F.grid_sample(\n",
    "                depth_map_tensor[indexes_i[start_idx:end_idx]].permute(0, 3, 1, 2),\n",
    "                corr_points_i_normalized[start_idx:end_idx, None],\n",
    "                align_corners=True,\n",
    "                mode='bilinear'\n",
    "            ).squeeze(1, 2))\n",
    "\n",
    "            depths_j_list.append(F.grid_sample(\n",
    "                depth_map_tensor[indexes_j[start_idx:end_idx]].permute(0, 3, 1, 2),\n",
    "                corr_points_j_normalized[start_idx:end_idx, None],\n",
    "                align_corners=True,\n",
    "                mode='bilinear'\n",
    "            ).squeeze(1, 2))\n",
    "        \n",
    "        depths_i = torch.cat(depths_i_list, dim=0).to(device).squeeze(-1)\n",
    "        depths_j = torch.cat(depths_j_list, dim=0).to(device).squeeze(-1) \n",
    "\n",
    "    qvec = qvec.to(device)\n",
    "    tvec = tvec.to(device)\n",
    "    log_sizes = log_sizes.to(device)\n",
    "    min_focals = min_focals.to(device)\n",
    "    max_focals = max_focals.to(device)\n",
    "    imsizes = imsizes.to(device)\n",
    "    pps = pps.to(device)\n",
    "    log_focals = log_focals.to(device)\n",
    "\n",
    "    corr_points_i = corr_points_i.to(device)\n",
    "    corr_points_j = corr_points_j.to(device)\n",
    "    corr_weight_valid = corr_weights.to(device)\n",
    "    # corr_weight_valid = corr_weight_valid**(0.5)\n",
    "\n",
    "    params = [{\n",
    "        \"params\": [\n",
    "            qvec.requires_grad_(True), \n",
    "            tvec.requires_grad_(True), \n",
    "            log_sizes.requires_grad_(True),\n",
    "        ],\n",
    "        \"name\": [\"qvec\", \"tvec\", \"log_sizes\"]\n",
    "    }]\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr=1, weight_decay=0, betas=(0.9, 0.9))\n",
    "\n",
    "    loss_list = []\n",
    "    for iter in tqdm(range(niter or 1), desc=\"Pose Optimization...\"):\n",
    "        K, (w2cam, cam2w) = make_K_cam_depth(log_focals, pps, tvec, qvec, min_focals, max_focals, imsizes)\n",
    "        \n",
    "        \n",
    "        alpha = (iter / niter)\n",
    "        lr = cosine_schedule(alpha, lr_base, lr_end)\n",
    "        adjust_learning_rate_by_lr(optimizer, lr)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Ks_i = K[indexes_i]\n",
    "        Ks_j = K[indexes_j]\n",
    "        w2cam_i = w2cam[indexes_i]\n",
    "        w2cam_j = w2cam[indexes_j]\n",
    "        cam2w_i = cam2w[indexes_i]\n",
    "        cam2w_j = cam2w[indexes_j]\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    "        sizes = log_sizes.exp()\n",
    "        global_scaling = 1 / sizes.min()\n",
    "        depths_i_scaled = depths_i * global_scaling * sizes[indexes_i, None]\n",
    "        depths_j_scaled = depths_j * global_scaling * sizes[indexes_j, None]\n",
    "        \n",
    "        cam_coords_i = torch.stack([\n",
    "            (corr_points_i[..., 0] - Ks_i[:, None, 0, 2]) / Ks_i[:, None, 0, 0],\n",
    "            (corr_points_i[..., 1] - Ks_i[:, None, 1, 2]) / Ks_i[:, None, 1, 1],\n",
    "            depths_i_scaled\n",
    "        ], dim=-1)\n",
    "        cam_coords_j = torch.stack([\n",
    "            (corr_points_j[..., 0] - Ks_j[:, None, 0, 2]) / Ks_j[:, None, 0, 0],\n",
    "            (corr_points_j[..., 1] - Ks_j[:, None, 1, 2]) / Ks_j[:, None, 1, 1],\n",
    "            depths_j_scaled\n",
    "        ], dim=-1)\n",
    "        world_coords_i = (cam2w_i[:, :3, :3] @ cam_coords_i.permute(0, 2, 1)).permute(0, 2, 1) + cam2w_i[:, None, :3, 3]\n",
    "        world_coords_j = (cam2w_j[:, :3, :3] @ cam_coords_j.permute(0, 2, 1)).permute(0, 2, 1) + cam2w_j[:, None, :3, 3]\n",
    "        \n",
    "        loss = ((world_coords_i - world_coords_j).abs() * corr_weight_valid).mean() * 0.05\n",
    "\n",
    "        P_i = Ks_i @ w2cam_i\n",
    "        P_j = Ks_j @ w2cam_j\n",
    "        Fm = kornia.geometry.epipolar.fundamental_from_projections(P_i[:, :3], P_j[:, :3])\n",
    "        err = kornia.geometry.symmetrical_epipolar_distance(corr_points_i, corr_points_j, Fm, squared=False, eps=1e-08)\n",
    "        loss = loss + (err * corr_weight_valid.squeeze(-1)).mean() * 1.0\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if target_scene_dir is not None:\n",
    "        plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(loss_list, label='Loss')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss Value')\n",
    "        plt.title(f'Loss Curve, final loss={loss_list[-1]:.4f}')\n",
    "        plt.show()\n",
    "        plt.savefig(f\"{target_scene_dir}/loss_curve_pose_opt.png\")\n",
    "\n",
    "    \n",
    "    output_extrinsic = w2cam[:, :3, :4].detach().cpu().numpy()\n",
    "    output_intrinsic = K[:, :3, :3].detach().cpu().numpy()\n",
    "\n",
    "    return output_extrinsic, output_intrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pose Optimization...: 100%|██████████| 300/300 [00:36<00:00,  8.24it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extrinsic_, intrinsic_ = pose_optimization(\n",
    "    output, extrinsic, intrinsic, images, depth_map, depth_conf,\n",
    "    base_image_path_list, target_scene_dir='./', shared_intrinsics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    --  Umeyama Scale:  0.1290909346903222\n",
      "    --  Umeyama Rotation: \n",
      " [[ 0.44253188  0.63722992 -0.63095445]\n",
      " [-0.23752481  0.76176131  0.60274519]\n",
      " [ 0.86472396 -0.11686661  0.48846155]]\n",
      "    --  Umeyama Translation: \n",
      " [[ 0.0258665 ]\n",
      " [-0.29517448]\n",
      " [ 0.47018299]]\n",
      "    --  Pair Rot   Error (Deg) of Vanilla:       0.69\n",
      "    --  Pair Trans Error (Deg) of Vanilla:       1.03\n",
      "    --  AUC at 30: 0.9779\n"
     ]
    }
   ],
   "source": [
    "# pred w2c\n",
    "pred_se3 = torch.eye(4, device=device).unsqueeze(0).repeat(len(extrinsic_), 1, 1)\n",
    "pred_se3[:, :3, :3] = torch.tensor(extrinsic_[:, :3, :3], device=device)\n",
    "pred_se3[:, 3, :3] = torch.tensor(extrinsic_[:, :3, 3], device=device)\n",
    "\n",
    "results = evaluate_auc(pred_se3, gt_se3, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3020948]\n"
     ]
    }
   ],
   "source": [
    "print(depth_map[255, 313, 444])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vggt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
